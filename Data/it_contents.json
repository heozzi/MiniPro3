[
    {
        "title": "DSA Tutorial - Learn Data Structures and Algorithms",
        "content": "DSA (D ata S tructures  and A lgorithms)  is the study of organizing data efficiently using data structures like arrays, stacks, and trees, paired with step-by-step procedures (or algorithms) to solve problems effectively. Data structures manage how data is stored and accessed, while algorithms focus on processing this data. Learning DSA boosts your problem-solving abilities and make you a stronger programmer. DSA is foundation for almost every software like GPS, Search Engines, AI ChatBots, Gaming Apps, Databases, Web Applications, etc Top Companies like Google, Microsoft, Amazon, Apple, Meta and many other heavily focus on DSA i n interviews. Learn at-least one programming language ( , , or ) and build your basic logic. Learn about Time and Space complexities Learn Data Structures and Algorithms Solve problems daily using weekly using and monthly using . Hoping you have learned a programming language of your choice, here comes the next stage of the roadmap - Learn about Time and Space Complexities. 1. Logic Building Once you have learned basics of a programming language, it is recommended that you learn basic logic building 2. Learn about Complexities To analyze algorithms, we mainly measure order of growth of time or space taken in terms of input size. We do this in the worst case scenario in most of the cases. Please refer the below links for a clear understanding of these concepts. 3. Array Array is a linear data structure where elements are allocated contiguous memory , allowing for constant-time access .  4. Searching Algorithms Searching algorithms are used to locate specific data within a large set of data. It helps find a target value within the data. There are various types of searching algorithms, each with its own approach and efficiency. 5. Sorting Algorithm Sorting algorithms are used to arrange the elements of a list in a specific order , such as numerical or alphabetical. It organizes the items in a systematic way, making it easier to search for and access specific elements.  6. Hashing Hashing is a technique that generates a fixed-size output (hash value) from an input of variable size using mathematical formulas called hash functions. Hashing is commonly used in data structures for efficient searching, insertion and deletion. 7. Two Pointer Technique I n Two Pointer Technique, we typically use two index variables from two corners of an array. We use the two pointer technique for searching a required point or value in an array. 8. Window Sliding Technique I n Window Sliding Technique, we use the result of previous subarray to quickly compute the result of current. 9. Prefix Sum Technique I n Prefix Sum Technique, we compute prefix sums of an array to quickly find results for a subarray. 10. String String is a sequence of characters, typically immutable and have limited set of elements (lower case or all English alphabets). 11. Recursion Recursion is a programming technique where a function calls itself within its own definition. It is usually used to solve problems that can be broken down into smaller instances of the same problem.  12. Matrix/Grid Matrix is a two-dimensional array of elements, arranged in rows and columns . It is represented as a rectangular grid, with each element at the intersection of a row and column. 13. Stack Stack is a linear data structure that follows the Last In, First Out (LIFO) principle. Stacks play an important role in managing function calls, memory, and are widely used in algorithms like stock span problem, next greater element and largest area in a histogram.  14. Queue Queue is a linear data structure that follows the First In, First Out (FIFO) principle. Queues play an important role in managing tasks or data in order, scheduling and message handling systems.  15. Linked List Linked list is a linear data structure that stores data in nodes, which are connected by pointers. Unlike arrays, nodes of linked lists are not stored in contiguous memory locations and can only be accessed sequentially , starting from the head of list.  16. Tree Tree is a non-linear, hierarchical data structure consisting of nodes connected by edges, with a top node called the root and nodes having child nodes. It is widely used in file systems , databases , decision-making algorithms , etc.  17. Heap Heap is a complete binary tree data structure that satisfies the heap property . Heaps are usually used to implement priority queues , where the smallest or largest element is always at the root of the tree.  18. Graph Graph is a non-linear data structure consisting of a finite set of vertices (or nodes) and a set of edges (or links)that connect a pair of nodes. Graphs are widely used to represent relationships between entities.  19. Greedy Algorithm Greedy Algorithm builds up the solution one piece at a time and chooses the next piece which gives the most obvious and immediate benefit i.e., which is the most optimal choice at that moment . So the problems where choosing locally optimal also leads to the global solutions are best fit for Greedy. 20. Dynamic Programming Dynamic Programming is a method used to solve complex problems by breaking them down into simpler subproblems . By solving each subproblem only once and storing the results , it avoids redundant computations, leading to more efficient solutions for a wide range of problems.  21. Other Algorithms Bitwise Algorithms: Operate on individual bits of numbers. Backtracking Algorithm : Follow Recursion  with the option to revert and traces back if the solution from current point is not feasible.  Divide and conquer: A strategy to solve problems by dividing them into smaller subproblems , solving those subproblems, and combining the solutions to obtain the final solution. Branch and Bound : Used in combinatorial optimization problems to systematically search for the best solution. It works by dividing the problem into smaller subproblems, or branches, and then eliminating certain branches based on bounds on the optimal solution. This process continues until the best solution is found or all branches have been explored. Geometric algorithms are a set of algorithms that solve problems related to shapes , points , lines and polygons. Randomized algorithms are algorithms that use randomness to solve problems. They make use of random input to achieve their goals, often leading to simpler and more efficient solutions . These algorithms may not product same result but are particularly useful in situations when a probabilistic approach is acceptable. 22. Advance Data Structure Advanced algorithms like Trie , Segment Tree , Red-Black Tree and Binary Indexed Tree offer significant performance improvements for specific problem domains. They provide efficient solutions for tasks like fast prefix searches, range queries, dynamic updates, and maintaining balanced data structures, which are crucial for handling large datasets and real-time processing. By optimizing time complexity, they enable better scalability and faster execution in many computational problems. Cheat Sheets Related Article:     "
    },
    {
        "title": "Data Structures Tutorial",
        "content": "Data structures are the fundamental building blocks of computer programming. They define how data is organized, stored, and manipulated within a program. Understanding data structures is very important for developing efficient and effective algorithms. In this tutorial, we will explore the most commonly used data structures, including arrays, linked lists, stacks, queues, trees, and graphs. A data structure is a storage that is used to store and organize data. It is a way of arranging data on a computer so that it can be accessed and updated efficiently. A data structure is not only used for organizing the data. It is also used for processing, retrieving, and storing data. There are different basic and advanced types of data structures that are used in almost every program or software system that has been developed. So we must have good knowledge about data structures.  Linear Data Structure:  Data structure in which data elements are arranged sequentially or linearly, where each element is attached to its previous and next adjacent elements, is called a linear data structure.  Example: Array, Stack, Queue, Linked List, etc. Static Data Structure:  Static data structure has a fixed memory size. It is easier to access the elements in a static data structure.  Example : array. Dynamic Data Structure:  In dynamic data structure, the size is not fixed. It can be randomly updated during the runtime which may be considered efficient concerning the memory (space) complexity of the code.  Example : Queue, Stack, etc. Non-Linear Data Structure:  Data structures where data elements are not placed sequentially or linearly are called non-linear data structures. In a non-linear data structure, we can\u2019t traverse all the elements in a single run only.  Examples: Trees and Graphs. Array  is a linear data structure that stores a collection of elements of the  same  data type. Elements are allocated  contiguous memory , allowing for  constant-time access . Each element has a unique  index  number. Matrix  is a two-dimensional array of elements, arranged in  rows  and  columns . It is represented as a rectangular grid, with each element at the intersection of a row and column. String  is a sequence of characters, typically used to represent  text . It is considered a data type that allows for the manipulation and processing of  textual data  in computer programs. Stack  is a linear data structure that follows the  Last In, First Out (LIFO)  principle. Stacks play an important role in managing function calls, memory, and are widely used in algorithms, web development, and systems like compilers and browsers. Queue  is a linear data structure that follows the  First In, First Out (FIFO)  principle. Queues play an important role in managing tasks or data in order, scheduling and message handling systems. Linked list  is a linear data structure that stores data in nodes, which are connected by pointers. Unlike arrays, nodes of linked lists are not stored in contiguous memory locations and can only be  accessed sequentially , starting from the head of list. Hashing  is a technique that generates a fixed-size output (hash value) from an input of variable size using mathematical formulas called  hash functions . Hashing is commonly used in data structures for efficient  searching ,  insertion  and  deletion  and plays a key role in software applications like  secure data retrieval ,  password storage ,  cryptography ,  digital signatures , etc. Tree  is a  non-linear, hierarchical  data structure consisting of nodes connected by edges, with a top node called the  root  and nodes having child nodes. It is widely used in  file systems ,  databases ,  decision-making algorithms , etc. Binary Tree  is a  non-linear  and  hierarchical  data structure where each node has at most two children referred to as the  left child  and the  right child . Binary Search Tree is a type of binary tree in which each node's left subtree contains only values smaller than the node, and each node's right subtree contains only values greater than the node. This property applies recursively, meaning that for every node, its left and right subtrees must also satisfy the conditions of a valid Binary Search Tree. Heap  is a  complete binary tree  data structure that satisfies the  heap property . Heaps are usually used to implement  priority queues , where the  smallest  or  largest  element is always at the root of the tree. Graph  is a  non-linear  data structure consisting of a finite set of  vertices (or nodes) and a set of  edges (or links)that connect a pair of nodes. Graphs are widely used to represent relationships between entities. Advanced Data Structures are complex arrangement of data which are used to organize, store, and manipulate data efficiently, enabling faster and more effective processing in complex algorithms. Unlike basic data types such as arrays and linked lists, these structures include more sophisticated options like Segment Trees, Trie, Binary Indexed Tree, Suffix Array etc. Related Articles:     "
    },
    {
        "title": "Array Data Structure Guide",
        "content": "In this article, we introduce array, implementation in different popular languages, its basic operations and commonly seen problems / interview questions. An array stores items (in case of C/C++ and Java Primitive Arrays) or their references (in case of Python, JS, Java Non-Primitive) at contiguous locations. It offers mainly the following advantages over other data structures. Random Access : i-th item can be accessed in O(1) Time as we have the base address and every item or reference is of same size. Cache Friendliness : Since items / references are stored at contiguous locations, we get the advantage of locality of reference. It is not useful in places where we have operations like insert in the middle, delete from middle and search in a unsorted data. It is a fundamental and linear data structure using which we build other data structures like Stack Queue, Deque, Graph, Hash Table, etc. , , , , , , and / / / [More problems on 2 Sum in Medium Section]      [More problems on 4 Sum in Hard Section]  Quick Links : Recommended:     "
    },
    {
        "title": "String in Data Structure",
        "content": "A string is a sequence of characters. The following facts make string an interesting data structure. Small set of elements. Unlike normal array, strings typically have smaller set of items. For example, lowercase English alphabet has only 26 characters. ASCII has only 256 characters. Strings are immutable in programming languages like Java, Python, JavaScript and C#. Many String Problems can optimized using the fact that the character set size is small. For example sorting can be done faster, counting frequencies of items is faster and many interesting interview questions are based on this.  Please refer String Problems Topic Wise  for the list of questions on different topics like binary string, subsequence and substring, pattern searching and palindrome.           "
    },
    {
        "title": "Linked List Data Structure",
        "content": "A linked list is a fundamental data structure in computer science. It mainly allows efficient insertion and deletion operations compared to . Like arrays, it is also used to implement other data structures like stack, queue and deque. Here\u2019s the comparison of Linked List vs Arrays Linked List: Data Structure: Non-contiguous Memory Allocation: Typically allocated one by one to individual elements Insertion/Deletion: Efficient Access: Sequential Array: Data Structure: Contiguous Memory Allocation: Typically allocated to the whole array Insertion/Deletion: Inefficient Access: Random                  .     Quick Links : Recommended:     "
    },
    {
        "title": "Stack Data Structure",
        "content": "A Stack is a linear data structure that follows a particular order in which the operations are performed. The order may be LIFO(Last In First Out) or FILO(First In Last Out) . LIFO implies that the element that is inserted last, comes out first and FILO implies that the element that is inserted first, comes out last. It behaves like a stack of plates, where the last plate added is the first one to be removed. Think of it this way: Pushing an element onto the stack is like adding a new plate on top. Popping an element removes the top plate from the stack.    Quick Links : Recommended: Learn Data Structure and Algorithms | DSA Tutorial     "
    },
    {
        "title": "Queue Data Structure",
        "content": "A Queue Data Structure is a fundamental concept in computer science used for storing and managing data in a specific order. It follows the principle of \" First in, First out \" (FIFO) , where the first element added to the queue is the first one to be removed. Queues are commonly used in various algorithms and applications for their simplicity and efficiency in managing data flow.  Quick Links: Recommended: Learn Data Structure and Algorithms | DSA Tutorial     "
    },
    {
        "title": "Tree Data Structure",
        "content": "Tree Data Structure is a non-linear data structure in which a collection of elements known as nodes are connected to each other via edges such that there exists exactly one path between any two nodes.   Ternary Tree: Interval Tree: 2-3-4 Tree: Problems Difficulty Level Solve Easy Easy Easy Easy Easy Easy Easy Easy Easy Easy Easy Easy Easy Easy Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium Hard Hard Hard Hard Hard Hard Hard Hard Hard Hard Quick Links:     "
    },
    {
        "title": "Binary Tree Data Structure",
        "content": "A Binary Tree Data Structure is a hierarchical data structure in which each node has at most two children, referred to as the left child and the right child. It is commonly used in computer science for efficient storage and retrieval of data, with various operations such as insertion, deletion, and traversal.     Quick Links : Recommended:     "
    },
    {
        "title": "Binary Search Tree",
        "content": "A Binary Search Tree (or BST) is a data structure used in computer science for organizing and storing data in a sorted manner. Each node in a Binary Search Tree has at most two children, a left child and a right child, with the left child containing values less than the parent node and the right child containing values greater than the parent node. This hierarchical structure allows for efficient searching , insertion , and deletion operations on the data stored in the tree. Some Quizzes: Quick Links : Recommended:     "
    },
    {
        "title": "Heap Data Structure",
        "content": "A Heap is a complete binary tree data structure that satisfies the heap property: for every node, the value of its children is greater than or equal to its own value. Heaps are usually used to implement priority queues, where the smallest (or largest) element is always at the root of the tree.    1 / 4    &    &          Quick Links: Recommended:     "
    },
    {
        "title": "Hashing in Data Structure",
        "content": "Hashing is a technique used in data structures that efficiently stores and retrieves data in a way that allows for quick access. Hashing involves mapping data to a specific index in a hash table (an array of items) using a hash function. It enables fast retrieval of information based on its key. The great thing about hashing is, we can achieve all three operations (search, insert and delete) in O(1) time on average. Hashing is mainly used to implement a set of distinct items (only keys) and dictionaries (key value pairs).      Quick Links : Recommended: Learn Data Structure and Algorithms | DSA Tutorial     "
    },
    {
        "title": "Graph Algorithms",
        "content": "Graph algorithms are methods used to manipulate and analyze graphs, solving various range of problems like finding the shortest path, cycles detection. If you are looking for difficulty-wise list of problems, please refer to  .   Quick Links : Recommended: Learn Data Structure and Algorithms | DSA Tutorial     "
    },
    {
        "title": "Trie Data Structure Tutorial",
        "content": "The trie data structure , also known as a prefix tree , is a tree-like data structure used for efficient retrieval of key-value pairs. It is commonly used for implementing dictionaries and autocomplete features, making it a fundamental component in many search algorithms. In this article, we will explore all about Trie data structures in detail. Trie data structure is defined as a Tree based data structure that is used for storing a collection of strings and performing efficient search, insert, delete, prefix search and sorted-traversal-of-all operations on them. The word Trie is derived from re TRIE val, which means finding something or obtaining it.  Trie data structure follows a property that If two strings have a common prefix then they will have the same ancestor in the trie. This particular property allows to find all words with a given prefix. A Trie data structure is used for storing and retrieval of data and the same operations could be done using another data structure which is Hash Table but Trie data structure can perform these operations more efficiently than a Hash Table. Moreover, Trie has its own advantage over the Hash table. A Trie data structure can be used for prefix-based searching and a sorted traversal of all words. So a Trie has advantages of both hash table and self balancing binary search trees. However the main issue with Trie is extra memory space required to store words and the space may become huge for long list of words and/or for long words. The A trie data structure has the following advantages over a hash table:   We can efficiently do prefix search (or auto-complete) with Trie. We can easily print all words in alphabetical order which is not easily possible with hashing. There is no overhead of Hash functions in a Trie data structure. Searching for a String even in the large collection of strings in a Trie data structure can be done in O(L) Time complexity, Where L is the number of words in the query string. This searching time could be even less than O(L) if the query string does not exist in the trie. Below are some important properties of the Trie data structure: Each Trie has an empty root node, with links (or references) to other nodes Each node of a Trie represents a string and each edge represents a character. Every node consists of hashmaps  or an array of pointers , with each index representing a character and a flag to indicate if any string ends at the current node. Trie data structure can contain any number of characters including alphabets , numbers , and special characters . But for this article, we will discuss strings with characters a-z. Therefore, only 26 pointers need for every node, where the 0th index represents \u2018a\u2019 and the 25th index represents \u2018z\u2019 characters. Each path from the root to any node represents a word or string. Below is a simple example of Trie data structure. Trie data structure can contain any number of characters including alphabets , numbers , and special characters . But for this article, we will discuss strings with characters a-z . Therefore, only 26 pointers need for every node, where the 0th index represents \u2018a\u2019 and the 25th index represents \u2018z\u2019 characters. Any lowercase English word can start with a-z , then the next letter of the word could be a-z, the third letter of the word again could be a-z , and so on. So for storing a word, we need to take an array (container) of size 26 and initially, all the characters are empty as there are no words and it will look as shown below. Let\u2019s see how a word \u201c and \u201d and \u201c ant \u201d is stored in the Trie data structure:  Store \u201c and \u201d in Trie data structure: The word \u201c and \u201d starts with \u201c a \u201c, So we will mark the position \u201c a \u201d as filled in the Trie node, which represents the use of \u201ca\u201d.  After placing the first character, for the second character again there are 26 possibilities , So from \u201c a \u201c, again there is an array of size 26 , for storing the 2nd character. The second character is \u201c n \u201c, So from \u201c a \u201c, we will move to \u201c n \u201d and mark \u201c n \u201d in the 2nd array as used. After \u201c n \u201c, the 3rd character is \u201c d \u201c, So mark the position \u201c d \u201d as used in the respective array. Store \u201c ant \u201d in the Trie data structure: The word \u201c ant \u201d starts with \u201c a \u201d and the position of \u201c a \u201d in the root node has already been filled. So, no need to fill it again, just move to the node \u2018 a \u2018 in Trie. For the second character \u2018 n \u2018 we can observe that the position of \u2018n\u2019 in the \u2018a\u2019 node has already been filled. So, no need to fill it again, just move to node \u2018n\u2019 in Trie. For the last character \u2018 t \u2018 of the word, The position for \u2018 t \u2018 in the \u2018 n \u2018 node is not filled. So, filled the position of \u2018 t \u2018 in \u2018 n \u2018 node and move to \u2018 t \u2018 node. After storing the word \u201cand\u201d and \u201cant\u201d the Trie will look like this: Every Trie node consists of a character pointer array or hashmap and a flag to represent if the word is ending at that node or not. But if the words contain only lower-case letters (i.e. a-z), then we can define Trie Node with an array instead of a hashmap.  struct   TrieNode   {      struct   TrieNode *   children [ ALPHABET_SIZE ];      // This will keep track of number of strings that are      // stored in the Trie from root node to any Trie node.      int   wordCount   =   0 ; };                                                                                                                                                                                                      Insertion Search Deletion This operation is used to insert new strings into the Trie data structure. Let us see how this works: Let us try to Insert \u201cand\u201d & \u201cant\u201d in this Trie: From the above representation of insertion, we can see that the word \u201c and \u201d & \u201c ant \u201d have shared some common node (i.e \u201can\u201d) this is because of the property of the Trie data structure that If two strings have a common prefix then they will have the same ancestor in the trie. Now let us try to Insert \u201cdad\u201d & \u201cdo\u201d: Implementation of Insertion in Trie data structure: Algorithm: Define a function insert(TrieNode *root, string &word) which will take two parameters one for the root and the other for the string that we want to insert in the Trie data structure. Now take another pointer currentNode and initialize it with the root node. Iterate over the length of the given string and check if the value is NULL or not in the array of pointers at the current character of the string. If It\u2019s NULL then, make a new node and point the current character to this newly created node. Move the curr to the newly created node. Finally, increment the wordCount of the last currentNode, this implies that there is a string ending currentNode. Below is the implementation of the above algorithm:  void   insert_key ( TrieNode *   root ,   string &   key ) {      // Initialize the currentNode pointer      // with the root node      TrieNode *   currentNode   =   root ;      // Iterate across the length of the string      for   ( auto   c   :   key )   {          // Check if the node exist for the current          // character in the Trie.          if   ( currentNode -> childNode [ c   -   'a' ]   ==   NULL )   {              // If node for current character does not exist              // then make a new node              TrieNode *   newNode   =   new   TrieNode ();              // Keep the reference for the newly created              // node.              currentNode -> childNode [ c   -   'a' ]   =   newNode ;          }          // Now, move the current node pointer to the newly          // created node.          currentNode   =   currentNode -> childNode [ c   -   'a' ];      }      // Increment the wordEndCount for the last currentNode      // pointer this implies that there is a string ending at      // currentNode.      currentNode -> wordCount ++ ; }                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Search operation in Trie is performed in a similar way as the insertion operation but the only difference is that whenever we find that the array of pointers in curr node does not point to the current character of the word then return false instead of creating a new node for that current character of the word.  This operation is used to search whether a string is present in the Trie data structure or not. There are two search approaches in the Trie data structure. Find whether the given word exists in Trie. Find whether any word that starts with the given prefix exists in Trie. There is a similar search pattern in both approaches. The first step in searching a given word in Trie is to convert the word to characters and then compare every character with the trie node from the root node. If the current character is present in the node, move forward to its children. Repeat this process until all characters are found. 2.1 Searching Prefix in Trie Data Structure: Search for the prefix \u201c an \u201d in the Trie Data Structure. Implementation of Prefix Search in Trie data structure:  bool   isPrefixExist ( TrieNode *   root ,   string &   key ) {      // Initialize the currentNode pointer      // with the root node      TrieNode *   currentNode   =   root ;      // Iterate across the length of the string      for   ( auto   c   :   key )   {          // Check if the node exist for the current          // character in the Trie.          if   ( currentNode -> childNode [ c   -   'a' ]   ==   NULL )   {                         // Given word as a prefix does not exist in Trie              return   false ;          }          // Move the currentNode pointer to the already           // existing node for current character.          currentNode   =   currentNode -> childNode [ c   -   'a' ];      }          // Prefix exist in the Trie      return   true ; }                                                                                                                                                                                                                                                                                                                                                                                                                   2.2 Searching Complete word in Trie Data Structure: It is similar to prefix search but additionally, we have to check if the word is ending at the last character of the word or not. Implementation of Search in Trie data structure:  bool   search_key ( TrieNode *   root ,   string &   key ) {      // Initialize the currentNode pointer      // with the root node      TrieNode *   currentNode   =   root ;      // Iterate across the length of the string      for   ( auto   c   :   key )   {          // Check if the node exist for the current          // character in the Trie.          if   ( currentNode -> childNode [ c   -   'a' ]   ==   NULL )   {                         // Given word does not exist in Trie              return   false ;          }          // Move the currentNode pointer to the already           // existing node for current character.          currentNode   =   currentNode -> childNode [ c   -   'a' ];      }        return   ( currentNode -> wordCount   >   0 ); }                                                                                                                                                                                                                                                                                                                                                                                                               This operation is used to delete strings from the Trie data structure. There are three cases when deleting a word from Trie. The deleted word is a prefix of other words in Trie. The deleted word shares a common prefix with other words in Trie. The deleted word does not share any common prefix with other words in Trie. 3.1 The deleted word is a prefix of other words in Trie. As shown in the following figure, the deleted word \u201c an \u201d share a complete prefix with another word \u201c and \u201d and \u201c ant \u201c. An easy solution to perform a delete operation for this case is to just decrement the wordCount by 1  at the ending node of the word. 3.2 The deleted word shares a common prefix with other words in Trie. As shown in the following figure, the deleted word \u201c and \u201d has some common prefixes with other words \u2018 ant \u2019. They share the prefix \u2018 an \u2019. The solution for this case is to delete all the nodes starting from the end of the prefix to the last character of the given word. 3.3 The deleted word does not share any common prefix with other words in Trie. As shown in the following figure, the word \u201c geek \u201d does not share any common prefix with any other words. The solution for this case is just to delete all the nodes. Below is the implementation that handles all the above cases:  bool   delete_key ( TrieNode *   root ,   string &   word ) {      TrieNode *   currentNode   =   root ;      TrieNode *   lastBranchNode   =   NULL ;      char   lastBranchChar   =   'a' ;      for   ( auto   c   :   word )   {          if   ( currentNode -> childNode [ c   -   'a' ]   ==   NULL )   {              return   false ;          }          else   {              int   count   =   0 ;              for   ( int   i   =   0 ;   i   <   26 ;   i ++ )   {                  if   ( currentNode -> childNode [ i ]   !=   NULL )                      count ++ ;              }              if   ( count   >   1 )   {                  lastBranchNode   =   currentNode ;                  lastBranchChar   =   c ;              }              currentNode   =   currentNode -> childNode [ c   -   'a' ];          }      }      int   count   =   0 ;      for   ( int   i   =   0 ;   i   <   26 ;   i ++ )   {          if   ( currentNode -> childNode [ i ]   !=   NULL )              count ++ ;      }      // Case 1: The deleted word is a prefix of other words      // in Trie.      if   ( count   >   0 )   {          currentNode -> wordCount -- ;          return   true ;      }      // Case 2: The deleted word shares a common prefix with      // other words in Trie.      if   ( lastBranchNode   !=   NULL )   {          lastBranchNode -> childNode [ lastBranchChar ]   =   NULL ;          return   true ;      }      // Case 3: The deleted word does not share any common      // prefix with other words in Trie.      else   {          root -> childNode [ word [ 0 ]]   =   NULL ;          return   true ;      } }                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Algorithm: Create a root node with the help of TrieNode() constructor. Store a collection of strings that we have to insert in the trie in a vector of strings say, arr . Inserting all strings in Trie  with the help of the insertkey() function, Search strings from searchQueryStrings with the help of search_key() function. Delete the strings present in the deleteQueryStrings with the help of delete_key .  #include   <bits/stdc++.h> using   namespace   std ; struct   TrieNode   {      // pointer array for child nodes of each node      TrieNode *   childNode [ 26 ];      int   wordCount ;      TrieNode ()      {          // constructor          // initialize the wordCnt variable with 0          // initialize every index of childNode array with          // NULL          wordCount   =   0 ;          for   ( int   i   =   0 ;   i   <   26 ;   i ++ )   {              childNode [ i ]   =   NULL ;          }      } }; void   insert_key ( TrieNode *   root ,   string &   key ) {      // Initialize the currentNode pointer      // with the root node      TrieNode *   currentNode   =   root ;      // Iterate across the length of the string      for   ( auto   c   :   key )   {          // Check if the node exist for the current          // character in the Trie.          if   ( currentNode -> childNode [ c   -   'a' ]   ==   NULL )   {              // If node for current character does not exist              // then make a new node              TrieNode *   newNode   =   new   TrieNode ();              // Keep the reference for the newly created              // node.              currentNode -> childNode [ c   -   'a' ]   =   newNode ;          }          // Now, move the current node pointer to the newly          // created node.          currentNode   =   currentNode -> childNode [ c   -   'a' ];      }      // Increment the wordEndCount for the last currentNode      // pointer this implies that there is a string ending at      // currentNode.      currentNode -> wordCount ++ ; } bool   search_key ( TrieNode *   root ,   string &   key ) {      // Initialize the currentNode pointer      // with the root node      TrieNode *   currentNode   =   root ;      // Iterate across the length of the string      for   ( auto   c   :   key )   {          // Check if the node exist for the current          // character in the Trie.          if   ( currentNode -> childNode [ c   -   'a' ]   ==   NULL )   {              // Given word does not exist in Trie              return   false ;          }          // Move the currentNode pointer to the already          // existing node for current character.          currentNode   =   currentNode -> childNode [ c   -   'a' ];      }      return   ( currentNode -> wordCount   >   0 ); } bool   delete_key ( TrieNode *   root ,   string &   word ) {      TrieNode *   currentNode   =   root ;      TrieNode *   lastBranchNode   =   NULL ;      char   lastBrachChar   =   'a' ;      for   ( auto   c   :   word )   {          if   ( currentNode -> childNode [ c   -   'a' ]   ==   NULL )   {              return   false ;          }          else   {              int   count   =   0 ;              for   ( int   i   =   0 ;   i   <   26 ;   i ++ )   {                  if   ( currentNode -> childNode [ i ]   !=   NULL )                      count ++ ;              }              if   ( count   >   1 )   {                  lastBranchNode   =   currentNode ;                  lastBrachChar   =   c ;              }              currentNode   =   currentNode -> childNode [ c   -   'a' ];          }      }      int   count   =   0 ;      for   ( int   i   =   0 ;   i   <   26 ;   i ++ )   {          if   ( currentNode -> childNode [ i ]   !=   NULL )              count ++ ;      }      // Case 1: The deleted word is a prefix of other words      // in Trie.      if   ( count   >   0 )   {          currentNode -> wordCount -- ;          return   true ;      }      // Case 2: The deleted word shares a common prefix with      // other words in Trie.      if   ( lastBranchNode   !=   NULL )   {          lastBranchNode -> childNode [ lastBrachChar ]   =   NULL ;          return   true ;      }      // Case 3: The deleted word does not share any common      // prefix with other words in Trie.      else   {          root -> childNode [ word [ 0 ]]   =   NULL ;          return   true ;      } } // Driver code int   main () {      // Make a root node for the Trie      TrieNode *   root   =   new   TrieNode ();      // Stores the strings that we want to insert in the      // Trie      vector < string >   inputStrings          =   {   \"and\" ,   \"ant\" ,   \"do\" ,   \"geek\" ,   \"dad\" ,   \"ball\"   };      // number of insert operations in the Trie      int   n   =   inputStrings . size ();      for   ( int   i   =   0 ;   i   <   n ;   i ++ )   {          insert_key ( root ,   inputStrings [ i ]);      }      // Stores the strings that we want to search in the Trie      vector < string >   searchQueryStrings          =   {   \"do\" ,   \"geek\" ,   \"bat\"   };      // number of search operations in the Trie      int   searchQueries   =   searchQueryStrings . size ();      for   ( int   i   =   0 ;   i   <   searchQueries ;   i ++ )   {          cout   <<   \"Query String: \"   <<   searchQueryStrings [ i ]               <<   \" \\n \" ;          if   ( search_key ( root ,   searchQueryStrings [ i ]))   {              // the queryString is present in the Trie              cout   <<   \"The query string is present in the \"                      \"Trie \\n \" ;          }          else   {              // the queryString is not present in the Trie              cout   <<   \"The query string is not present in \"                      \"the Trie \\n \" ;          }      }      // stores the strings that we want to delete from the      // Trie      vector < string >   deleteQueryStrings   =   {   \"geek\" ,   \"tea\"   };      // number of delete operations from the Trie      int   deleteQueries   =   deleteQueryStrings . size ();      for   ( int   i   =   0 ;   i   <   deleteQueries ;   i ++ )   {          cout   <<   \"Query String: \"   <<   deleteQueryStrings [ i ]               <<   \" \\n \" ;          if   ( delete_key ( root ,   deleteQueryStrings [ i ]))   {              // The queryString is successfully deleted from              // the Trie              cout   <<   \"The query string is successfully \"                      \"deleted \\n \" ;          }          else   {              // The query string is not present in the Trie              cout   <<   \"The query string is not present in \"                      \"the Trie \\n \" ;          }      }      return   0 ; }                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Output Operation Time Complexity Insertion O(n) Here n is the length of string to be searched Searching O(n) Deletion O(n) Note: In the above complexity table \u2018 n\u2019 , \u2018 m\u2019 represents the size of the string and the number of strings that are stored in the trie. 1. Autocomplete Feature: Autocomplete provides suggestions based on what you type in the search box. Trie data structure is used to implement autocomplete functionality.   2. Spell Checkers: If the word typed does not appear in the dictionary, then it shows suggestions based on what you typed. It is a 3-step process that includes : Checking for the word in the data dictionary. Generating potential suggestions. Sorting the suggestions with higher priority on top. Trie stores the data dictionary and makes it easier to build an algorithm for searching the word from the dictionary and provides the list of valid words for the suggestion. 3. Longest Prefix Matching Algorithm(Maximum Prefix Length Match): This algorithm is used in networking by the routing devices in IP networking. Optimization of network routes requires contiguous masking that bound the complexity of lookup a time to O(n), where n is the length of the URL address in bits. To speed up the lookup process, Multiple Bit trie schemes were developed that perform the lookups of multiple bits faster. Trie allows us to input and finds words in O(n) time, where n is the length of a single word. It is faster as compared to both hash tables and binary search trees. It provides alphabetical filtering of entries by the key of the node and hence makes it easier to print all words in alphabetical order. Prefix search/Longest prefix matching can be efficiently done with the help of trie data structure. Since trie doesn\u2019t need any hash function for its implementation so they are generally faster than hash tables for small keys like integers and pointers. Tries support ordered iteration whereas iteration in a hash table will result in pseudorandom order given by the hash function which is usually more cumbersome. Deletion is also a straightforward algorithm with O(n) as its time complexity, where n is the length of the word to be deleted. The main disadvantage of the trie is that it takes a lot of memory to store all the strings. For each node, we have too many node pointers which are equal to the no of characters in the worst case. An efficiently constructed hash table(i.e. a good hash function and a reasonable load factor) has O(1) as lookup time which is way faster than O(l) in the case of a trie, where l is the length of the string. S.no Problem Practice 1 2 3 4 5 6 7 8 Is trie an advanced data structure? A Trie is an advanced data structure that is sometimes also known as a prefix tree What is the difference between trie and tree data structure? A tree is a general structure of recursive nodes. There are many types of trees. Popular ones are the binary tree and balanced tree. A Trie is a kind of tree, known by many names including prefix tree, digital search tree, and retrieval tree (hence the name \u2018trie\u2019). What are some applications of Trie data structure? The longest common prefix, pattern searching, autocomplete and implementation of the dictionary are some of the common applications of a Trie Data Structure. Does Google use trie data structure? Google even stores each word/sentence in the form of a trie. What is the disadvantage of trie data structure? The main disadvantage of Trie is that it takes a lot of memory to store all the Strings. For each node, we have too many node pointers (equal to the number of characters of the alphabet). Our discussion so far has led us to the conclusion that the Trie data structure is a Tree based data structure that is used for storing some collection of strings and performing efficient search operations on them and we have also discussed the various advantage and applications of trie data structure. Related articles:     "
    },
    {
        "title": "Segment Tree",
        "content": "Segment Tree is a data structures that allows efficient querying and updating of intervals or segments of an array. It is particularly useful for problems involving range queries, such as finding the sum, minimum, maximum, or any other operation over a specific range of elements in an array. The tree is built recursively by dividing the array into segments until each segment represents a single element. This structure enables fast query and update operations with a time complexity of O(log n) , making it a powerful tool in algorithm design and optimization . The operations that the segment tree can perform must be  binary and associative . Some of the examples of operations are: Finding Range Sum Queries Searching index with given prefix sum Finding Range Maximum/Minimum Counting frequency of Range Maximum/Minimum Finding Range GCD/LCM Finding Range AND/OR/XOR Finding number of zeros in the given range or finding index of Kth zero Interval scheduling:  Segment trees can be used to efficiently schedule non-overlapping intervals, such as scheduling appointments or allocating resources. Range-based statistics:  Segment trees can be used to compute range-based statistics such as variance, standard deviation, and percentiles. Image processing:  Segment trees are used in image processing algorithms to divide an image into segments based on color, texture, or other attributes.     "
    },
    {
        "title": "Introduction to Disjoint Set (Union-Find Algorithm)",
        "content": "Two sets are called disjoint sets if they don\u2019t have any element in common. The disjoint set data structure is used to store such sets. It supports following operations: Merging two disjoint sets to a single set using Union operation. Finding representative of a disjoint set using Find operation. Check if two elements belong to same set or not. We mainly find representative of both and check if same. Consider a situation with a number of persons and the following tasks to be performed on them: Add a new friendship  relation , i.e. a person x becomes the friend of another person y i.e adding new element to a set. Find whether individual x is a friend of individual y (direct or indirect friend) Examples:  We are given 10 individuals say, a, b, c, d, e, f, g, h, i, j Following are relationships to be added: a <-> b   b <-> d c <-> f c <-> i j <-> e g <-> j Given queries like whether a is a friend of d or not. We basically need to create following 4 groups and maintain a quickly accessible connection among group items: G1 = {a, b, d} G2 = {c, f, i} G3 = {e, g, j} G4 = {h} Find whether x and y belong to the same group or not, i.e. to find if x and y are direct/indirect friends. Partitioning the individuals into different sets according to the groups in which they fall. This method is known as a Disjoint set Union which maintains a collection of Disjoint sets and each set is represented by one of its members. To answer the above question two key points to be considered are: How to Resolve sets? Initially, all elements belong to different sets. After working on the given relations, we select a member as a representative . Check if 2 persons are in the same group? If representatives of two individuals are the same, then they are friends. Data Structures used are:   Array: An array of integers is called Parent[] . If we are dealing with N items, i\u2019th element of the array represents the i\u2019th item. More precisely, the i\u2019th element of the Parent[] array is the parent of the i\u2019th item. These relationships create one or more virtual trees. Tree: It is a Disjoint set . If two elements are in the same tree, then they are in the same Disjoint set . The root node (or the topmost node) of each tree is called the representative of the set. There is always a single unique representative of each set. A simple rule to identify a representative is if \u2018i\u2019 is the representative of a set, then Parent[i] = i . If i is not the representative of his set, then it can be found by traveling up the tree until we find the representative. 1. Find: The task is to find representative of the set of a given element. The representative is always root of the tree. So we implement find() by recursively traversing the parent array until we hit a node that is root (parent of itself). 2. Union:   The task is to combine two sets and make one. It takes two elements as input and finds the representatives of their sets using the Find operation, and finally puts either one of the trees (representing the set) under the root node of the other tree.  #include   <iostream> #include   <vector> using   namespace   std ; class   UnionFind   {      vector < int >   parent ; public :      UnionFind ( int   size )   {                 parent . resize ( size );                 // Initialize the parent array with each           // element as its own representative          for   ( int   i   =   0 ;   i   <   size ;   i ++ )   {              parent [ i ]   =   i ;          }      }      // Find the representative (root) of the      // set that includes element i      int   find ( int   i )   {                 // If i itself is root or representative          if   ( parent [ i ]   ==   i )   {              return   i ;          }                 // Else recursively find the representative           // of the parent          return   find ( parent [ i ]);      }      // Unite (merge) the set that includes element       // i and the set that includes element j      void   unite ( int   i ,   int   j )   {                 // Representative of set containing i          int   irep   =   find ( i );                 // Representative of set containing j          int   jrep   =   find ( j );                  // Make the representative of i's set          // be the representative of j's set          parent [ irep ]   =   jrep ;      } }; int   main ()   {      int   size   =   5 ;      UnionFind   uf ( size );      uf . unite ( 1 ,   2 );      uf . unite ( 3 ,   4 );      bool   inSameSet   =   ( uf . find ( 1 )   ==   uf . find ( 2 ));      cout   <<   \"Are 1 and 2 in the same set? \"             <<   ( inSameSet   ?   \"Yes\"   :   \"No\" )   <<   endl ;      return   0 ; }                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Output The above and are naive and the worst case time complexity is linear. The trees created to represent subsets can be skewed and can become like a linked list. Following is an example of worst case scenario.     1 / 4 Optimization (Path Compression and Union by Rank/Size): The main idea is to reduce heights of trees representing different sets. We achieve this with two most common methods: 1) Path Compression 2) Union By Rank Path Compression (Used to improve find()): The idea is to flatten the tree when is called. When is called for an element x, root of the tree is returned. The operation traverses up from x to find root. The idea of path compression is to make the found root as parent of x so that we don\u2019t have to traverse all intermediate nodes again. If x is root of a subtree, then path (to root) from all nodes under x also compresses.    1 / 3 It speeds up the data structure by compressing the height of the trees. It can be achieved by inserting a small caching mechanism into the Find operation. Take a look at the code for more details: Union by Rank (Modifications to union()) Rank is like height of the trees representing different sets.  We use an extra  array of integers called rank[] . The size of this array is the same as the parent array Parent[] . If i is a representative of a set, rank[i] is the rank of the element i.  Rank is same as height if path compression is not used. With path compression, rank can be more than the actual height. Now recall that in the Union operation, it doesn\u2019t matter which of the two trees is moved under the other. Now what we want to do is minimize the height of the resulting tree. If we are uniting two trees (or sets), let\u2019s call them left and right, then it all depends on the rank of left and the rank of right .  If the rank of left is less than the rank of right , then it\u2019s best to move left under right , because that won\u2019t change the rank of right (while moving right under left would increase the height). In the same way, if the rank of right is less than the rank of left, then we should move right under left. If the ranks are equal, it doesn\u2019t matter which tree goes under the other, but the rank of the result will always be one greater than the rank of the trees.    1 / 4  #include   <iostream> #include   <vector> using   namespace   std ; class   DisjointUnionSets   {      vector < int >   rank ,   parent ; public :         // Constructor to initialize sets      DisjointUnionSets ( int   n )   {          rank . resize ( n ,   0 );          parent . resize ( n );          // Initially, each element is in its own set          for   ( int   i   =   0 ;   i   <   n ;   i ++ )   {              parent [ i ]   =   i ;          }      }      // Find the representative of the set that x belongs to      int   find ( int   i )   {          int   root   =   parent [ i ];                 if   ( parent [ root ]   !=   root )   {              return   parent [ i ]   =   find ( root );          }                 return   root ;      }      // Union of sets containing x and y      void   unionSets ( int   x ,   int   y )   {          int   xRoot   =   find ( x );          int   yRoot   =   find ( y );          // If they are in the same set, no need to union          if   ( xRoot   ==   yRoot )   return ;          // Union by rank          if   ( rank [ xRoot ]   <   rank [ yRoot ])   {              parent [ xRoot ]   =   yRoot ;          }   else   if   ( rank [ yRoot ]   <   rank [ xRoot ])   {              parent [ yRoot ]   =   xRoot ;          }   else   {              parent [ yRoot ]   =   xRoot ;              rank [ xRoot ] ++ ;          }      } }; int   main ()   {      // Let there be 5 persons with ids 0, 1, 2, 3, and 4      int   n   =   5 ;      DisjointUnionSets   dus ( n );      // 0 is a friend of 2      dus . unionSets ( 0 ,   2 );      // 4 is a friend of 2      dus . unionSets ( 4 ,   2 );      // 3 is a friend of 1      dus . unionSets ( 3 ,   1 );      // Check if 4 is a friend of 0      if   ( dus . find ( 4 )   ==   dus . find ( 0 ))          cout   <<   \"Yes \\n \" ;      else          cout   <<   \"No \\n \" ;      // Check if 1 is a friend of 0      if   ( dus . find ( 1 )   ==   dus . find ( 0 ))          cout   <<   \"Yes \\n \" ;      else          cout   <<   \"No \\n \" ;      return   0 ; }                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Output Time complexity : O(n) for creating n single item sets . The two techniques -path compression with the union by rank/size, the time complexity will reach nearly constant time. It turns out, that the final  is O(\u03b1(n)), where \u03b1(n) is the inverse Ackermann function, which grows very steadily (it does not even exceed for n<10 600   approximately). Space complexity: O(n) because we need to store n elements in the Disjoint Set Data Structure. Union by Size (Alternate of Union by Rank) We use an array of integers called size[] . The size of this array is the same as the parent array Parent[] . If i is a representative of a set, size[i] is the number of the elements in the tree representing the set.  Now we are uniting two trees (or sets), let\u2019s call them left and right, then in this case it all depends on the size of left and the size of right tree (or set). If the size of left is less than the size of right , then it\u2019s best to move left under right and increase size of right by size of left. In the same way, if the size of right is less than the size of left, then we should move right under left. and increase size of left by size of right. If the sizes are equal, it doesn\u2019t matter which tree goes under the other.  // C++ program for Union by Size with Path Compression #include   <iostream> #include   <vector> using   namespace   std ; class   UnionFind   {      vector < int >   Parent ;      vector < int >   Size ; public :      UnionFind ( int   n )   {                 Parent . resize ( n );          for   ( int   i   =   0 ;   i   <   n ;   i ++ )   {              Parent [ i ]   =   i ;          }          // Initialize Size array with 1s          Size . resize ( n ,   1 );      }      // Function to find the representative (or the root      // node) for the set that includes i      int   find ( int   i )   {          int   root   =   Parent [ i ];                 if   ( Parent [ root ]   !=   root )   {              return   Parent [ i ]   =   find ( root );          }                 return   root ;      }      // Unites the set that includes i and the set that      // includes j by size      void   unionBySize ( int   i ,   int   j )   {                 // Find the representatives (or the root nodes) for          // the set that includes i          int   irep   =   find ( i );          // And do the same for the set that includes j          int   jrep   =   find ( j );          // Elements are in the same set, no need to unite          // anything.          if   ( irep   ==   jrep )              return ;          // Get the size of i\u2019s tree          int   isize   =   Size [ irep ];          // Get the size of j\u2019s tree          int   jsize   =   Size [ jrep ];          // If i\u2019s size is less than j\u2019s size          if   ( isize   <   jsize )   {                         // Then move i under j              Parent [ irep ]   =   jrep ;              // Increment j's size by i's size              Size [ jrep ]   +=   Size [ irep ];          }          // Else if j\u2019s size is less than i\u2019s size          else   {              // Then move j under i              Parent [ jrep ]   =   irep ;              // Increment i's size by j's size              Size [ irep ]   +=   Size [ jrep ];          }      } }; int   main ()   {      int   n   =   5 ;      UnionFind   unionFind ( n );      unionFind . unionBySize ( 0 ,   1 );      unionFind . unionBySize ( 2 ,   3 );      unionFind . unionBySize ( 0 ,   4 );      for   ( int   i   =   0 ;   i   <   n ;   i ++ )   {          cout   <<   \"Element \"   <<   i   <<   \": Representative = \"                 <<   unionFind . find ( i )   <<   endl ;      }      return   0 ; }                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Output     "
    },
    {
        "title": "Binary Indexed Tree or Fenwick Tree",
        "content": "Binary Indexed Trees are used for problems where we have following types of multiple operations on a fixed sized. Prefix Operation (Sum, Product, XOR, OR, etc). Note that range operations can also be solved using prefix. For example, range sum from index L to R is prefix sum till R (included minus prefix sum til L-1. Update an array item Time Complexities of both the operations is O(Log n). Note that we need O(n Log n) preprocessing time and O(n) auxiliary space. Example Problem to Understand Binary Index Tree Let us consider the following problem to understand Binary Indexed Tree. We have an array arr[0 . . . n-1]. We would like to  Compute the sum of the first i elements.  Modify the value of a specified element of the array arr[i] = x where 0 <= i <= n-1. A simple solution is to run a loop from 0 to i-1 and calculate the sum of the elements. To update a value, simply do arr[i] = x. The first operation takes O(n) time and the second operation takes O(1) time. Another simple solution is to create an extra array and store the sum of the first i-th elements at the i-th index in this new array. The sum of a given range can now be calculated in O(1) time, but the update operation takes O(n) time now. This works well if there are a large number of query operations but a very few number of update operations. Could we perform both the query and update operations in O(log n) time?  One efficient solution is to use that performs both operations in O(Logn) time. . Representation   Binary Indexed Tree is represented as an array. Let the array be BITree[]. Each node of the Binary Indexed Tree stores the sum of some elements of the input array. The size of the Binary Indexed Tree is equal to the size of the input array, denoted as n. In the code below, we use a size of n+1 for ease of implementation. Construction   We initialize all the values in BITree[] as 0. Then we call update() for all the indexes, the update() operation is discussed below. Operations    getSum(x): Returns the sum of the sub-array arr[0,\u2026,x]   // Returns the sum of the sub-array arr[0,\u2026,x] using BITree[0..n], which is constructed from arr[0..n-1]  1) Initialize the output sum as 0, the current index as x+1.  2) Do following while the current index is greater than 0.  \u2026a) Add BITree[index] to sum  \u2026b) Go to the parent of BITree[index]. The parent can be obtained by removing  the last set bit from the current index, i.e., index = index \u2013 (index & (-index))  3) Return sum.  The diagram above provides an example of how getSum() is working. Here are some important observations. BITree[0] is a dummy node.  BITree[y] is the parent of BITree[x], if and only if y can be obtained by removing the last set bit from the binary representation of x, that is y = x \u2013 (x & (-x)). The child node BITree[x] of the node BITree[y] stores the sum of the elements between y(inclusive) and x(exclusive): arr[y,\u2026,x).    update(x, val): Updates the Binary Indexed Tree (BIT) by performing arr[index] += val   // Note that the update(x, val) operation will not change arr[]. It only makes changes to BITree[]  1) Initialize the current index as x+1.  2) Do the following while the current index is smaller than or equal to n.  \u2026a) Add the val to BITree[index]  \u2026b) Go to next element of BITree[index]. The next element can be obtained by incrementing the last set bit of the current index, i.e., index = index + (index & (-index))   The update function needs to make sure that all the BITree nodes which contain arr[i] within their ranges being updated. We loop over such nodes in the BITree by repeatedly adding the decimal number corresponding to the last set bit of the current index. How does Binary Indexed Tree work?   The idea is based on the fact that all positive integers can be represented as the sum of powers of 2. For example 19 can be represented as 16 + 2 + 1. Every node of the BITree stores the sum of n elements where n is a power of 2. For example, in the first diagram above (the diagram for getSum()), the sum of the first 12 elements can be obtained by the sum of the last 4 elements (from 9 to 12) plus the sum of 8 elements (from 1 to 8). The number of set bits in the binary representation of a number n is O(Logn). Therefore, we traverse at-most O(Logn) nodes in both getSum() and update() operations. The time complexity of the construction is O(nLogn) as it calls update() for all n elements.  Implementation:   Following are the implementations of Binary Indexed Tree.    // C++ code to demonstrate operations of Binary Index Tree #include   <iostream> using   namespace   std ; /*         n --> No. of elements present in input array.      BITree[0..n] --> Array that represents Binary Indexed Tree.     arr[0..n-1] --> Input array for which prefix sum is evaluated. */ // Returns sum of arr[0..index]. This function assumes // that the array is preprocessed and partial sums of // array elements are stored in BITree[]. int   getSum ( int   BITree [],   int   index ) {      int   sum   =   0 ;   // Initialize result      // index in BITree[] is 1 more than the index in arr[]      index   =   index   +   1 ;      // Traverse ancestors of BITree[index]      while   ( index > 0 )      {          // Add current element of BITree to sum          sum   +=   BITree [ index ];          // Move index to parent node in getSum View          index   -=   index   &   ( - index );      }      return   sum ; } // Updates a node in Binary Index Tree (BITree) at given index // in BITree. The given value 'val' is added to BITree[i] and  // all of its ancestors in tree. void   updateBIT ( int   BITree [],   int   n ,   int   index ,   int   val ) {      // index in BITree[] is 1 more than the index in arr[]      index   =   index   +   1 ;      // Traverse all ancestors and add 'val'      while   ( index   <=   n )      {      // Add 'val' to current node of BI Tree      BITree [ index ]   +=   val ;      // Update index to that of parent in update View      index   +=   index   &   ( - index );      } } // Constructs and returns a Binary Indexed Tree for given // array of size n. int   * constructBITree ( int   arr [],   int   n ) {      // Create and initialize BITree[] as 0      int   * BITree   =   new   int [ n + 1 ];      for   ( int   i = 1 ;   i <= n ;   i ++ )          BITree [ i ]   =   0 ;      // Store the actual values in BITree[] using update()      for   ( int   i = 0 ;   i < n ;   i ++ )          updateBIT ( BITree ,   n ,   i ,   arr [ i ]);      // Uncomment below lines to see contents of BITree[]      //for (int i=1; i<=n; i++)      //     cout << BITree[i] << \" \";      return   BITree ; } // Driver program to test above functions int   main () {      int   freq []   =   { 2 ,   1 ,   1 ,   3 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 };      int   n   =   sizeof ( freq ) / sizeof ( freq [ 0 ]);      int   * BITree   =   constructBITree ( freq ,   n );      cout   <<   \"Sum of elements in arr[0..5] is \"          <<   getSum ( BITree ,   5 );      // Let use test the update operation      freq [ 3 ]   +=   6 ;      updateBIT ( BITree ,   n ,   3 ,   6 );   //Update BIT for above change in arr[]      cout   <<   \" \\n Sum of elements in arr[0..5] after update is \"          <<   getSum ( BITree ,   5 );      return   0 ; }                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Output Time Complexity: O(NLogN) Auxiliary Space: O(N) Can we extend the Binary Indexed Tree to computing the sum of a range in O(Logn) time?   Yes. rangeSum(l, r) = getSum(r) \u2013 getSum(l-1). Applications:   The implementation of the arithmetic coding algorithm. The development of the Binary Indexed Tree was primarily motivated by its application in this case. See for more details. Example Problems:         References:         "
    },
    {
        "title": "AVL Tree Data Structure",
        "content": "An AVL tree defined as a self-balancing where the difference between heights of left and right subtrees for any node cannot be more than one. The absolute difference between the heights of the left subtree and the right subtree for any node is known as the balance factor of the node. The balance factor for all nodes must be less than or equal to 1. Every AVL tree is also a Binary Search Tree (Left subtree values Smaller and Right subtree values grater for every node), but every BST is not AVL Tree. For example, the second diagram below is not an AVL Tree. The main advantage of an AVL Tree is, the time complexities of all operations (search, insert and delete, max, min, floor and ceiling) become O(Log n). This happens because height of an AVL tree is bounded by O(Log n). In case of a normal BST, the height can go up to O(n). An AVL tree maintains its height by doing some extra work during insert and delete operations. It mainly uses rotations to maintain both BST properties and height balance. There exist other self-balancing BSTs also like . Red Black tree is more complex, but used more in practice as it is less restrictive in terms of left and right subtree height differences. Example of an AVL Tree: The balance factors for different nodes are : 12 :1, 8:1, 18:1, 5:1, 11:0, 17:0 and 4:0. Since all differences are less than or equal to 1, the tree is an AVL tree. Example of a BST which is NOT AVL: The Below Tree is NOT an AVL Tree as t he balance factor for nodes 8, 4 and 7 is more than 1. Operations on an AVL Tree: Searching : It is same as normal Binary Search Tree (BST) as an AVL Tree is always a BST. So we can use the same implementation as BST. The advantage here is time complexity is O(Log n) Insertion : It does rotations along with normal BST insertion to make sure that the balance factor of the impacted nodes is less than or equal to 1 after insertion Deletion : It also does rotations along with normal BST deltion to make sure that the balance factor of the impacted nodes is less than or equal to 1 after deletion. Please refer and for details. Rotating the subtrees (Used in Insertion and Deletion) An AVL tree may rotate in one of the following four ways to keep itself balanced while making sure that the BST properties are maintained. Left Rotation : When a node is added into the right subtree of the right subtree, if the tree gets out of balance, we do a single left rotation. Right Rotation : If a node is added to the left subtree of the left subtree, the AVL tree may get out of balance, we do a single right rotation. Left-Right Rotation : A left-right rotation is a combination in which first left rotation takes place after that right rotation executes. Right-Left Rotation : A right-left rotation is a combination in which first right rotation takes place after that left rotation executes. Advantages of AVL Tree: AVL trees can self-balance themselves and therefore provides time complexity as O(Log n) for search, insert and delete. It is a BST only (with balancing), so items can be traversed in sorted order. Since the balancing rules are strict compared to , AVL trees in general have relatively less height and hence the search is faster. AVL tree is relatively less complex to understand and implement compared to Red Black Trees. Disadvantages of AVL Tree: It is difficult to implement compared to normal BST and easier compared to Red Black Less used compared to Red-Black trees. Due to its rather strict balance, AVL trees provide complicated insertion and removal operations as more rotations are performed. Applications of AVL Tree: AVL Tree is used as a first example self balancing BST in teaching DSA as it is easier to understand and implement compared to Red Black Applications, where insertions and deletions are less common but frequent data lookups along with other operations of BST like sorted traversal, floor, ceil, min and max. Red Black tree is more commonly implemented in language libraries like , , and . AVL Trees can be used in a real time environment where predictable and consistent performance is required. Related Articles:     "
    },
    {
        "title": "Introduction to Red-Black Tree",
        "content": "Binary search trees are a fundamental but their performance can suffer if the tree becomes unbalanced. Red Black Trees are a type of balanced binary search tree that use a set of rules to maintain balance, ensuring logarithmic time complexity for operations like insertion, deletion, and searching , regardless of the initial shape of the tree. Red Black Trees are self-balancing, using a simple color-coding scheme to adjust the tree after each modification. A Red-Black Tree is a self-balancing where each node has an additional attribute: a color, which can be either red or black . The primary objective of these trees is to maintain balance during insertions and deletions, ensuring efficient data retrieval and manipulation. A have the following properties: Node Color : Each node is either red or black . Root Property : The root of the tree is always black . Red Property : Red nodes cannot have red children (no two consecutive red nodes on any path). Black Property : Every path from a node to its descendant null nodes (leaves) has the same number of black nodes. Leaf Property : All leaves (NIL nodes) are black . These properties ensure that the longest path from the root to any leaf is no more than twice as long as the shortest path, maintaining the tree\u2019s balance and efficient performance. The Correct Red-Black Tree in above image ensures that every path from the root to a leaf node has the same number of black nodes. In this case, there is one (excluding the root node). The Incorrect Red Black Tree does not follow the red-black properties as two red nodes are adjacent to each other. Another problem is that one of the paths to a leaf node has zero black nodes, whereas the other two contain a black node. Most of the BST operations (e.g., search, max, min, insert, delete.. etc) take O(h) time where h is the height of the . The cost of these operations may become O(n) for a skewed If we make sure that the height of the tree remains O(log n) after every insertion and deletion, then we can guarantee an upper bound of O(log n) for all these operations. The height of a Red-Black tree is always O(log n) where n is the number of nodes in the tree.  Sr. No. Algorithm Time Complexity 1. Search O(log n) 2. Insert O(log n) 3. Delete O(log n) The AVL trees are more balanced compared to Red-Black Trees, but they may cause more rotations during insertion and deletion. So if your application involves frequent insertions and deletions, then Red-Black trees should be preferred. And if the insertions and deletions are less frequent and search is a more frequent operation, then should be preferred over the Red-Black Tree. How does a Red-Black Tree ensure balance? A simple example to understand balancing is, that a chain of 3 nodes is not possible in the Red-Black tree. We can try any combination of colors and see if all of them violate the Red-Black tree property.  The black height of the red-black tree is the number of black nodes on a path from the root node to a leaf node. Leaf nodes are also counted as black nodes. So, a red-black tree of height h has black height >= h/2 . Height of a red-black tree with n nodes is h<= 2 log 2 (n + 1) . All leaves (NIL) are black . The black depth of a node is defined as the number of black nodes from the root to that node i.e the number of black ancestors. The basic operations on a Red-Black Tree include: Insertion Search Deletion Rotation Inserting a new node in a Red-Black Tree involves a two-step process: performing a standard , followed by fixing any violations of Red-Black properties. Insertion Steps BST Insert : Insert the new node like in a standard BST. Fix Violations : If the parent of the new node is black , no properties are violated. If the parent is red , the tree might violate the Red Property, requiring fixes. Fixing Violations During Insertion After inserting the new node as a red node, we might encounter several cases depending on the colors of the node\u2019s parent and uncle (the sibling of the parent): Case 1: Uncle is Red : Recolor the parent and uncle to black , and the grandparent to red . Then move up the tree to check for further violations. Case 2: Uncle is Black : Sub-case 2.1: Node is a right child : Perform a left rotation on the parent. Sub-case 2.2: Node is a left child : Perform a right rotation on the grandparent and recolor appropriately. Searching for a node in a Red-Black Tree is similar to searching in a standard Binary Search Tree (BST) . The search operation follows a straightforward path from the root to a leaf , comparing the target value with the current node\u2019s value and moving left or right accordingly. Search Steps Start at the Root : Begin the search at the root node. Traverse the Tree : If the target value is equal to the current node\u2019s value, the node is found. If the target value is less than the current node\u2019s value, move to the left child. If the target value is greater than the current node\u2019s value, move to the right child. Repeat : Continue this process until the target value is found or a NIL node is reached (indicating the value is not present in the tree). Deleting a node from a Red-Black Tree also involves a two-step process: performing the BST deletion, followed by fixing any violations that arise. Deletion Steps BST Deletion : Remove the node using standard BST rules. Fix Double Black : If a black node is deleted, a \u201cdouble black\u201d condition might arise, which requires specific fixes. Fixing Violations During Deletion When a black node is deleted, we handle the double black issue based on the sibling\u2019s color and the colors of its children: Case 1: Sibling is Red : Rotate the parent and recolor the sibling and parent. Case 2: Sibling is Black : Sub-case 2.1: Sibling\u2019s children are black : Recolor the sibling and propagate the double black upwards. Sub-case 2.2: At least one of the sibling\u2019s children is red : If the sibling\u2019s far child is red : Perform a rotation on the parent and sibling, and recolor appropriately. If the sibling\u2019s near child is red : Rotate the sibling and its child, then handle as above. Rotations are fundamental operations in maintaining the balanced structure of a Red-Black Tree (RBT). They help to preserve the properties of the tree, ensuring that the longest path from the root to any leaf is no more than twice the length of the shortest path. Rotations come in two types: left rotations and right rotations. 1. Left Rotation A left rotation at node \ud835\udc65 moves \ud835\udc65 down to the left and its right child \ud835\udc66 up to take \ud835\udc65 \u2019s place.  Before   Rotation :      x                                                      \\                                                      y                                                                 /   \\                                                            a     b                                                       After   Left   Rotation :        y       /        x     b              a Left Rotation Steps: Set to be the right child of . Move \u2019s left subtree to \u2019s right subtree. Update the parent of and . Update \u2019s parent to point to instead of . Set \u2019s left child to . Update \u2019s parent to . Pseudocode of Left Rotation:  // Utility function to perform left rotation void   leftRotate ( Node *   x ) {      Node *   y   =   x -> right ;      x -> right   =   y -> left ;      if   ( y -> left   !=   NIL )   {          y -> left -> parent   =   x ;      }      y -> parent   =   x -> parent ;      if   ( x -> parent   ==   nullptr )   {          root   =   y ;      }      else   if   ( x   ==   x -> parent -> left )   {          x -> parent -> left   =   y ;      }      else   {          x -> parent -> right   =   y ;      }      y -> left   =   x ;      x -> parent   =   y ; } 2. Right Rotation A right rotation at node \ud835\udc65 moves \ud835\udc65 down to the right and its left child \ud835\udc66 up to take \ud835\udc65 \u2019s place.  Befor   Right   Rotation :             x       /      y     /      a     b After   Right   Rotation :      y     /      a     x       /      b Right Rotation Steps: Set y to be the left child of x . Move \u2019s right subtree to \u2019s left subtree. Update the parent of x and y . Update \u2019s parent to point to y instead of x . Set \u2019s right child to . Update \u2019s parent to . Pseudocode of Right Rotation:  // Utility function to perform right rotation void   rightRotate ( Node *   x ) {      Node *   y   =   x -> left ;      x -> left   =   y -> right ;      if   ( y -> right   !=   NIL )   {          y -> right -> parent   =   x ;      }      y -> parent   =   x -> parent ;      if   ( x -> parent   ==   nullptr )   {          root   =   y ;      }      else   if   ( x   ==   x -> parent -> right )   {          x -> parent -> right   =   y ;      }      else   {          x -> parent -> left   =   y ;      }      y -> right   =   x ;      x -> parent   =   y ; } Rotations in Red-Black Trees are typically performed during insertions and deletions to maintain the properties of the tree. Below are the scenarios for rotations: 1. Fixing Violations after Insertion When a new node is inserted, it is always colored red. This can create violations of Red-Black Tree properties, specifically: The root must be black . Red nodes cannot have red children. Case Analysis for Fixing Insertions: Case 1: Recoloring and Propagating Upwards If the parent and uncle of the new node are both red , recolor the parent and uncle to black , and the grandparent to red . Then, recursively apply the fix-up to the grandparent. Case 2: Rotation and Recoloring If the new node\u2019s uncle is black and the new node is the right child of a left child (or vice versa), perform a rotation to move the new node up and align it. If the new node\u2019s uncle is black and the new node is the left child of a left child (or right of a right), perform a rotation and recolor the parent and grandparent to fix the violation. 2. Fixing Violations after Deletion After deletion, the tree might need fixing to restore properties: When a black node is removed, or a red node is replaced by a black node, a double-black situation can arise. Case Analysis for Fixing Deletions: Case 1: Sibling is Red Recolor the sibling and the parent, and perform a rotation. Case 2: Sibling is Black with Black Children Recolor the sibling to red and move the problem up to the parent. Case 3: Sibling is Black with at least one Red Child Rotate and recolor to fix the double-black issue. Here\u2019s a detailed implementation of a Red-Black Tree including insertion, search, and rotation functions:  #include   <iostream> using   namespace   std ; // Node structure for the Red-Black Tree struct   Node   {      int   data ;      string   color ;      Node   * left ,   * right ,   * parent ;      Node ( int   data )          :   data ( data )          ,   color ( \"RED\" )          ,   left ( nullptr )          ,   right ( nullptr )          ,   parent ( nullptr )      {      } }; // Red-Black Tree class class   RedBlackTree   { private :      Node *   root ;      Node *   NIL ;      // Utility function to perform left rotation      void   leftRotate ( Node *   x )      {          Node *   y   =   x -> right ;          x -> right   =   y -> left ;          if   ( y -> left   !=   NIL )   {              y -> left -> parent   =   x ;          }          y -> parent   =   x -> parent ;          if   ( x -> parent   ==   nullptr )   {              root   =   y ;          }          else   if   ( x   ==   x -> parent -> left )   {              x -> parent -> left   =   y ;          }          else   {              x -> parent -> right   =   y ;          }          y -> left   =   x ;          x -> parent   =   y ;      }      // Utility function to perform right rotation      void   rightRotate ( Node *   x )      {          Node *   y   =   x -> left ;          x -> left   =   y -> right ;          if   ( y -> right   !=   NIL )   {              y -> right -> parent   =   x ;          }          y -> parent   =   x -> parent ;          if   ( x -> parent   ==   nullptr )   {              root   =   y ;          }          else   if   ( x   ==   x -> parent -> right )   {              x -> parent -> right   =   y ;          }          else   {              x -> parent -> left   =   y ;          }          y -> right   =   x ;          x -> parent   =   y ;      }      // Function to fix Red-Black Tree properties after      // insertion      void   fixInsert ( Node *   k )      {          while   ( k   !=   root   &&   k -> parent -> color   ==   \"RED\" )   {              if   ( k -> parent   ==   k -> parent -> parent -> left )   {                  Node *   u   =   k -> parent -> parent -> right ;   // uncle                  if   ( u -> color   ==   \"RED\" )   {                      k -> parent -> color   =   \"BLACK\" ;                      u -> color   =   \"BLACK\" ;                      k -> parent -> parent -> color   =   \"RED\" ;                      k   =   k -> parent -> parent ;                  }                  else   {                      if   ( k   ==   k -> parent -> right )   {                          k   =   k -> parent ;                          leftRotate ( k );                      }                      k -> parent -> color   =   \"BLACK\" ;                      k -> parent -> parent -> color   =   \"RED\" ;                      rightRotate ( k -> parent -> parent );                  }              }              else   {                  Node *   u   =   k -> parent -> parent -> left ;   // uncle                  if   ( u -> color   ==   \"RED\" )   {                      k -> parent -> color   =   \"BLACK\" ;                      u -> color   =   \"BLACK\" ;                      k -> parent -> parent -> color   =   \"RED\" ;                      k   =   k -> parent -> parent ;                  }                  else   {                      if   ( k   ==   k -> parent -> left )   {                          k   =   k -> parent ;                          rightRotate ( k );                      }                      k -> parent -> color   =   \"BLACK\" ;                      k -> parent -> parent -> color   =   \"RED\" ;                      leftRotate ( k -> parent -> parent );                  }              }          }          root -> color   =   \"BLACK\" ;      }      // Inorder traversal helper function      void   inorderHelper ( Node *   node )      {          if   ( node   !=   NIL )   {              inorderHelper ( node -> left );              cout   <<   node -> data   <<   \" \" ;              inorderHelper ( node -> right );          }      }      // Search helper function      Node *   searchHelper ( Node *   node ,   int   data )      {          if   ( node   ==   NIL   ||   data   ==   node -> data )   {              return   node ;          }          if   ( data   <   node -> data )   {              return   searchHelper ( node -> left ,   data );          }          return   searchHelper ( node -> right ,   data );      } public :      // Constructor      RedBlackTree ()      {          NIL   =   new   Node ( 0 );          NIL -> color   =   \"BLACK\" ;          NIL -> left   =   NIL -> right   =   NIL ;          root   =   NIL ;      }      // Insert function      void   insert ( int   data )      {          Node *   new_node   =   new   Node ( data );          new_node -> left   =   NIL ;          new_node -> right   =   NIL ;          Node *   parent   =   nullptr ;          Node *   current   =   root ;          // BST insert          while   ( current   !=   NIL )   {              parent   =   current ;              if   ( new_node -> data   <   current -> data )   {                  current   =   current -> left ;              }              else   {                  current   =   current -> right ;              }          }          new_node -> parent   =   parent ;          if   ( parent   ==   nullptr )   {              root   =   new_node ;          }          else   if   ( new_node -> data   <   parent -> data )   {              parent -> left   =   new_node ;          }          else   {              parent -> right   =   new_node ;          }          if   ( new_node -> parent   ==   nullptr )   {              new_node -> color   =   \"BLACK\" ;              return ;          }          if   ( new_node -> parent -> parent   ==   nullptr )   {              return ;          }          fixInsert ( new_node );      }      // Inorder traversal      void   inorder ()   {   inorderHelper ( root );   }      // Search function      Node *   search ( int   data )      {          return   searchHelper ( root ,   data );      } }; int   main () {      RedBlackTree   rbt ;      // Inserting elements      rbt . insert ( 10 );      rbt . insert ( 20 );      rbt . insert ( 30 );      rbt . insert ( 15 );      // Inorder traversal      cout   <<   \"Inorder traversal:\"   <<   endl ;      rbt . inorder ();   // Output: 10 15 20 30      // Search for a node      cout   <<   \" \\n Search for 15: \"           <<   ( rbt . search ( 15 )   !=   rbt . search ( 0 ))           <<   endl ;   // Output: 1 (true)      cout   <<   \"Search for 25: \"           <<   ( rbt . search ( 25 )   !=   rbt . search ( 0 ))           <<   endl ;   // Output: 0 (false)      return   0 ; } Balanced: Red-Black Trees are self-balancing, meaning they automatically maintain a balance between the heights of the left and right subtrees. This ensures that search, insertion, and deletion operations take O(log n) time in the worst case. Efficient search, insertion, and deletion: Due to their balanced structure, Red-Black Trees offer efficient operations. Search, insertion, and deletion all take O(log n) time in the worst case. Simple to implement: The rules for maintaining the Red-Black Tree properties are relatively simple and straightforward to implement. Widely used: Red-Black Trees are a popular choice for implementing various data structures, such as maps, sets, and priority queues. More complex than other balanced trees: Compared to simpler balanced trees like AVL trees, Red-Black Trees have more complex insertion and deletion rules. Constant overhead: Maintaining the Red-Black Tree properties adds a small overhead to every insertion and deletion operation. Not optimal for all use cases: While efficient for most operations, Red-Black Trees might not be the best choice for applications where frequent insertions and deletions are required, as the constant overhead can become significant. Implementing maps and sets: Red-Black Trees are often used to implement maps and sets, where efficient search, insertion, and deletion are crucial. Priority queues: Red-Black Trees can be used to implement priority queues, where elements are ordered based on their priority. File systems: Red-Black Trees are used in some file systems to manage file and directory structures. In-memory databases: Red-Black Trees are sometimes used in in-memory databases to store and retrieve data efficiently. Graphics and game development: Red-Black Trees can be used in graphics and game for tasks like collision detection and pathfinding. 1. What is a Red-Black Tree? A Red-Black Tree is a self-balancing binary search tree that maintains a balance between the heights of its left and right subtrees. This ensures that search, insertion, and deletion operations take O(log n) time in the worst case. Red-Black Trees are widely used in various applications where efficient data structures are required. 2. How does a Red-Black Tree maintain its balance? Red-Black Trees maintain their balance by enforcing specific rules on the colors of nodes (RED or BLACK) and the relationships between them. These rules ensure that the tree remains balanced and that the height difference between the left and right subtrees is at most 1. 3. What are the advantages of using a Red-Black Tree? Balanced:  Red-Black Trees are self-balancing, ensuring efficient search, insertion, and deletion operations. Efficient:  They offer O(log n) time complexity for most operations. Simple to implement:  The rules for maintaining Red-Black Tree properties are relatively straightforward. Widely used:  They are a popular choice for implementing various data structures and algorithms. 4. What are the disadvantages of using a Red-Black Tree? Compared to simpler balanced trees like AVL trees, Red-Black Trees have more complex insertion and deletion rules. Maintaining the Red-Black Tree properties adds a small overhead to every insertion and deletion operation. For applications with frequent insertions and deletions, other balanced tree structures might be more suitable. 5. What are some common applications of Red-Black Trees? Implementing maps and sets Priority queues File systems In-memory databases Graphics and game development (collision detection, pathfinding) Related Articles:     "
    }
]